{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5c954fa-c632-4f46-b414-5fd51bd3ee8b",
   "metadata": {},
   "source": [
    "## 1. Cite 5 diferenças entre o Random Forest e o AdaBoost:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fe41f2-83ad-4ce5-a36e-d157caaec796",
   "metadata": {},
   "source": [
    "#### Método de combinação dos modelos:\n",
    "\n",
    "- **Random Forest:** Usa Bagging, onde várias árvores de decisão são criadas em paralelo e suas previsões são combinadas (por votação ou média) para criar a previsão final.\n",
    "- **AdaBoost:** Usa Boosting, onde os modelos são treinados de forma sequencial. Cada novo modelo corrige os erros do modelo anterior.\n",
    "\n",
    "#### Pesos dos exemplos:\n",
    "\n",
    "- **Random Forest:** Cada árvore usa uma amostra aleatória dos dados (com reposição), sem alterar os pesos dos exemplos.\n",
    "- **AdaBoost:** Dá mais peso aos exemplos que foram classificados incorretamente, focando em melhorar os erros em cada rodada de treinamento.\n",
    "\n",
    "#### Tamanho das árvores:\n",
    "\n",
    "- **Random Forest:** As árvores de decisão podem ser bastante profundas, pois cada árvore é treinada de forma independente.\n",
    "- **AdaBoost:** As árvores são geralmente rasas (muitas vezes chamadas de \"stumps\", com apenas 1 ou 2 níveis) para que cada modelo seja simples.\n",
    "\n",
    "#### Correção de erros:\n",
    "\n",
    "- **Random Forest:** As árvores são treinadas de forma independente, sem foco específico nos erros das outras.\n",
    "- **AdaBoost:** Cada novo modelo é treinado para corrigir os erros do modelo anterior, aumentando o peso dos exemplos mal classificados.\n",
    "\n",
    "#### Resistência a overfitting:\n",
    "\n",
    "- **Random Forest:** É menos propenso ao overfitting devido ao processo de Bagging e à aleatoriedade na escolha das variáveis.\n",
    "- **AdaBoost:** Pode ser mais suscetível ao overfitting se não for bem regulado, especialmente com dados ruidosos ou difíceis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2430ef91-87ee-444a-bcf1-9e4c1c0df497",
   "metadata": {},
   "source": [
    "## 2. Exemplo do AdaBoost.\n",
    "https://scikit-learn.org/stable/modules/ensemble.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "31fe6238-4315-4000-8000-84f6497ff0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets        import load_iris\n",
    "from sklearn.ensemble        import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6bc8a015-36e1-4861-8d27-ec29579c3853",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9533333333333334\n"
     ]
    }
   ],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Ajustar o AdaBoostClassifier para usar o algoritmo SAMME\n",
    "clf = AdaBoostClassifier(n_estimators=100, algorithm='SAMME')\n",
    "\n",
    "# Realizar validação cruzada com 5 folds\n",
    "scores = cross_val_score(estimator=clf, X=X, y=y, cv=5)\n",
    "\n",
    "# Exibir a média dos scores\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c831fffb-0b32-49a5-b3a5-5e0f575ae96a",
   "metadata": {},
   "source": [
    "## 3. Cite 5 hiperparâmetros importantes no AdaBoost:\n",
    "#### n_estimators:\n",
    "- O número de modelos (ou \"stumps\") que serão treinados de forma sequencial. Representa quantas vezes o processo de AdaBoost será repetido. Um número maior geralmente melhora o desempenho, mas pode aumentar o tempo de treinamento e o risco de overfitting.\n",
    "\n",
    "#### learning_rate:\n",
    "- Controla o quanto cada modelo contribui para o conjunto final. Valores menores reduzem a contribuição de cada modelo, fazendo o algoritmo ser mais conservador, mas exigindo mais iterações (n_estimators maiores).\n",
    "\n",
    "#### base_estimator:\n",
    "- O modelo base usado no AdaBoost. Por padrão, é uma árvore de decisão rasa (ou \"stump\"), mas você pode substituir por outros modelos como SVM, KNN, etc. A escolha do estimador base impacta diretamente no comportamento do AdaBoost.\n",
    "\n",
    "#### algorithm:\n",
    "Existem duas opções principais para o AdaBoost:\n",
    "\n",
    "- SAMME: É a versão multiclass do AdaBoost. Funciona bem para classificação com múltiplas classes.\n",
    "- SAMME.R: É uma variante mais rápida e eficiente que utiliza probabilidades nas previsões. Funciona melhor com classificadores probabilísticos, como árvores de decisão. Em resumo, o SAMME.R é mais comumente usado porque tende a ser mais eficiente em termos computacionais.\n",
    "#### random_state: \n",
    "- Usado para garantir que o processo seja reprodutível, controlando a aleatoriedade da amostragem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f104a8-6188-4d0b-ade0-0bf82df4acc1",
   "metadata": {},
   "source": [
    "## 4. Utilize o GridSearch para encontrar os melhores hyperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f63368-e246-460b-b69f-e85de4f97d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "model = AdaBoostClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 1.0],\n",
    "    'algorithm': ['SAMME', 'SAMME.R']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=5, \n",
    "                           scoring='accuracy', \n",
    "                           n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Melhores parâmetros encontrados:\", best_params)\n",
    "\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Acurácia do melhor modelo no conjunto de teste:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
